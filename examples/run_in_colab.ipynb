{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b6c9e5",
   "metadata": {},
   "source": [
    "# SOFTKILL-9000: Multi-Agent Motion Capture Simulation\n",
    "\n",
    "This notebook demonstrates how to run SOFTKILL-9000 in Google Colab.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BkAsDrP/Softkill9000/blob/main/examples/run_in_colab.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "SOFTKILL-9000 is a multi-agent reinforcement learning system for motion capture simulation. This notebook will:\n",
    "\n",
    "1. Install the package from GitHub\n",
    "2. Run a basic simulation\n",
    "3. Visualize agent trajectories\n",
    "4. Demonstrate Q-learning training\n",
    "5. Show advanced customization options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687a26b",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install SOFTKILL-9000 directly from the GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b50f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "‚úÖ SOFTKILL-9000 package installed!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install the package from GitHub\n",
    "!pip install git+https://github.com/BkAsDrP/Softkill9000.git -q\n",
    "print(\"‚úÖ SOFTKILL-9000 package installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b96739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "‚úÖ NumPy 2.x installed\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: If using this notebook locally:\n",
      "   1. Restart your Jupyter kernel to apply NumPy upgrade\n",
      "   2. In VS Code: Click the 'Restart' button in the kernel toolbar\n",
      "   3. Then continue with the verification cell below\n",
      "\n",
      "üí° Note: Auto-restart only works in Google Colab\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Upgrade NumPy to resolve dependency conflicts\n",
    "# Colab comes with NumPy 1.x, but softkill9000 needs NumPy 2.0.x for modern ML libraries\n",
    "# Pin to <2.1.0 for compatibility with numba, tensorflow, opencv, and cupy\n",
    "!pip install --upgrade \"numpy>=2.0.0,<2.1.0\" -q\n",
    "\n",
    "print(\"‚úÖ NumPy 2.x installed\")\n",
    "print(\"\")\n",
    "\n",
    "# Detect if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Auto-restart only in Colab\n",
    "    print(\"üîÑ Auto-restarting Colab runtime in 3 seconds...\")\n",
    "    print(\"   (This prevents binary incompatibility errors)\")\n",
    "    \n",
    "    import time\n",
    "    import os\n",
    "    time.sleep(3)\n",
    "    os.kill(os.getpid(), 9)\n",
    "else:\n",
    "    # Local environment - just show a message\n",
    "    print(\"‚ö†Ô∏è  IMPORTANT: If using this notebook locally:\")\n",
    "    print(\"   1. Restart your Jupyter kernel to apply NumPy upgrade\")\n",
    "    print(\"   2. In VS Code: Click the 'Restart' button in the kernel toolbar\")\n",
    "    print(\"   3. Then continue with the verification cell below\")\n",
    "    print(\"\")\n",
    "    print(\"üí° Note: Auto-restart only works in Google Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30de9cf",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Restart Required\n",
    "\n",
    "**If running in Google Colab**: The runtime will automatically restart after the cell above.\n",
    "\n",
    "**If running locally** (VS Code, JupyterLab, etc.): Manually restart your kernel now:\n",
    "- VS Code: Click the **Restart** button in the kernel toolbar\n",
    "- JupyterLab: Click **Kernel ‚Üí Restart Kernel**\n",
    "- Jupyter Notebook: Click **Kernel ‚Üí Restart**\n",
    "\n",
    "After restarting, continue with the verification cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e279bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SOFTKILL-9000 v1.0.0 ready!\n",
      "‚úÖ NumPy 2.3.4 (compatible with JAX, OpenCV, PyTensor)\n",
      "\n",
      "üéâ Installation complete and verified! Continue with examples below.\n"
     ]
    }
   ],
   "source": [
    "# Verification (Run this AFTER runtime restart)\n",
    "try:\n",
    "    import softkill9000\n",
    "    import numpy as np\n",
    "    \n",
    "    print(f\"‚úÖ SOFTKILL-9000 v{softkill9000.__version__} ready!\")\n",
    "    print(f\"‚úÖ NumPy {np.__version__} (compatible with JAX, OpenCV, PyTensor)\")\n",
    "    \n",
    "    # Test that numpy actually works (catches binary incompatibility)\n",
    "    test_array = np.random.rand(5)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"üéâ Installation complete and verified! Continue with examples below.\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    if \"numpy.dtype size changed\" in str(e):\n",
    "        print(\"‚ùå Binary incompatibility detected!\")\n",
    "        print(\"\")\n",
    "        print(\"SOLUTION: Manually restart the runtime:\")\n",
    "        print(\"   1. Click 'Runtime ‚Üí Restart runtime' in the menu\")\n",
    "        print(\"   2. Re-run this cell after restart\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec9d7b",
   "metadata": {},
   "source": [
    "## 2. Basic Simulation\n",
    "\n",
    "Let's run a basic simulation with default configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59852e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Agent.__init__() got an unexpected keyword argument 'agent_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m np.random.seed(\u001b[32m42\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create agents\u001b[39;00m\n\u001b[32m     10\u001b[39m agents = [\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLongsight\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrole\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscout\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m     12\u001b[39m     Agent(agent_id=\u001b[33m\"\u001b[39m\u001b[33mLifebinder\u001b[39m\u001b[33m\"\u001b[39m, role=\u001b[33m\"\u001b[39m\u001b[33mmedic\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     13\u001b[39m     Agent(agent_id=\u001b[33m\"\u001b[39m\u001b[33mSpecter\u001b[39m\u001b[33m\"\u001b[39m, role=\u001b[33m\"\u001b[39m\u001b[33massault\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m ]\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Create scenario\u001b[39;00m\n\u001b[32m     17\u001b[39m scenario = CosmicScenario.generate_random(\n\u001b[32m     18\u001b[39m     width=\u001b[32m100.0\u001b[39m,\n\u001b[32m     19\u001b[39m     height=\u001b[32m100.0\u001b[39m,\n\u001b[32m     20\u001b[39m     num_objectives=\u001b[32m3\u001b[39m,\n\u001b[32m     21\u001b[39m     num_obstacles=\u001b[32m5\u001b[39m\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: Agent.__init__() got an unexpected keyword argument 'agent_id'"
     ]
    }
   ],
   "source": [
    "from softkill9000.simulator import MissionSimulator\n",
    "from softkill9000.config.models import SimulationConfig, AgentConfig, MissionConfig\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create simulation configuration with custom agents\n",
    "config = SimulationConfig(\n",
    "    agents=[\n",
    "        AgentConfig(role=\"Longsight\", species=\"Vyr'khai\"),\n",
    "        AgentConfig(role=\"Lifebinder\", species=\"Lumenari\"),\n",
    "        AgentConfig(role=\"Brawler\", species=\"Aetherborn\")\n",
    "    ],\n",
    "    mission=MissionConfig(\n",
    "        num_timesteps=20,\n",
    "        ethics_enabled=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Initialize and run simulator\n",
    "simulator = MissionSimulator(config=config)\n",
    "simulator.setup()\n",
    "\n",
    "# Run simulation\n",
    "print(\"üöÄ Starting simulation...\\n\")\n",
    "results = simulator.run()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä SIMULATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüåå Scenario: {results['scenario']['description']}\")\n",
    "print(f\"üìç Location: {results['scenario']['planet']} in {results['scenario']['galaxy']}\")\n",
    "print(f\"üåç Terrain: {results['scenario']['terrain']}\")\n",
    "print(f\"üå§Ô∏è  Weather: {results['scenario']['weather']}\")\n",
    "print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "print(f\"  - Timesteps: {results['config']['num_timesteps']}\")\n",
    "print(f\"  - Ethics Mode: {'ENABLED' if results['config']['ethics_enabled'] else 'DISABLED'}\")\n",
    "print(f\"  - Q-Learning Episodes: {results['config']['q_learning_episodes']}\")\n",
    "print(f\"\\nüéØ Final Rewards:\")\n",
    "for role, reward in results['final_rewards'].items():\n",
    "    print(f\"  - {role}: {reward:.2f}\")\n",
    "print(f\"\\nüìà Total Squad Reward: {sum(results['final_rewards'].values()):.2f}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804a461",
   "metadata": {},
   "source": [
    "## 3. Visualize Agent Trajectories\n",
    "\n",
    "Plot the paths taken by each agent during the mission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from softkill9000.visualization.plots import create_reward_curve\n",
    "\n",
    "# Plot reward progression over time\n",
    "fig = create_reward_curve(results['reward_history'])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Reward Summary:\")\n",
    "for role, rewards in results['reward_history'].items():\n",
    "    total_reward = rewards[-1] if rewards else 0\n",
    "    avg_reward_per_tick = total_reward / len(rewards) if rewards else 0\n",
    "    print(f\"  {role}:\")\n",
    "    print(f\"    Total: {total_reward:.2f}\")\n",
    "    print(f\"    Avg per tick: {avg_reward_per_tick:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd28fe",
   "metadata": {},
   "source": [
    "## 4. Performance Metrics Over Time\n",
    "\n",
    "Visualize how agent stats evolved during the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ffac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from softkill9000.visualization.plots import create_radar_chart\n",
    "\n",
    "# Display agent statistics\n",
    "print(\"üìä Agent Statistics:\")\n",
    "print(\"=\"*60)\n",
    "for role, stats in results['agent_stats'].items():\n",
    "    print(f\"\\n{role} ({stats['species']}):\")\n",
    "    print(f\"  Strength:     {stats['strength']:3d}\")\n",
    "    print(f\"  Empathy:      {stats['empathy']:3d}\")\n",
    "    print(f\"  Intelligence: {stats['intelligence']:3d}\")\n",
    "    print(f\"  Mobility:     {stats['mobility']:3d}\")\n",
    "    print(f\"  Tactical:     {stats['tactical']:3d}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create radar chart of agent capabilities\n",
    "fig = create_radar_chart(results['agent_stats'], title=\"Squad Capabilities Radar\")\n",
    "plt.show()\n",
    "\n",
    "# Plot final rewards as a bar chart\n",
    "roles = list(results['final_rewards'].keys())\n",
    "rewards = list(results['final_rewards'].values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(roles, rewards, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "plt.xlabel('Agent Role')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Final Agent Rewards')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd94bb",
   "metadata": {},
   "source": [
    "## 5. Q-Learning Training\n",
    "\n",
    "Train agents using reinforcement learning to improve their decision-making:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softkill9000.simulator import MissionSimulator\n",
    "from softkill9000.config.models import SimulationConfig, AgentConfig, MissionConfig, QLearningConfig\n",
    "\n",
    "# Configure simulation with more Q-learning episodes for better training\n",
    "training_config = SimulationConfig(\n",
    "    agents=[\n",
    "        AgentConfig(role=\"Longsight\", species=\"Vyr'khai\"),\n",
    "        AgentConfig(role=\"Lifebinder\", species=\"Lumenari\"),\n",
    "        AgentConfig(role=\"Specter\", species=\"Zephryl\")\n",
    "    ],\n",
    "    mission=MissionConfig(\n",
    "        num_timesteps=30,\n",
    "        ethics_enabled=True\n",
    "    ),\n",
    "    q_learning=QLearningConfig(\n",
    "        episodes=2000,  # More training episodes\n",
    "        gamma=0.95,\n",
    "        alpha=0.3,\n",
    "        epsilon=0.2\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run simulation with enhanced training\n",
    "print(\"üéì Training agents with Q-Learning...\\n\")\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  - Episodes: {training_config.q_learning.episodes}\")\n",
    "print(f\"  - Learning Rate (alpha): {training_config.q_learning.alpha}\")\n",
    "print(f\"  - Discount Factor (gamma): {training_config.q_learning.gamma}\")\n",
    "print(f\"  - Exploration Rate (epsilon): {training_config.q_learning.epsilon}\")\n",
    "print()\n",
    "\n",
    "simulator = MissionSimulator(config=training_config)\n",
    "simulator.setup()  # This trains the Q-learning model\n",
    "results = simulator.run()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä TRAINING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüåå Scenario: {results['scenario']['description']}\")\n",
    "print(f\"üìç Location: {results['scenario']['planet']} in {results['scenario']['galaxy']}\")\n",
    "print(f\"\\nüéØ Final Rewards After Training:\")\n",
    "for role, reward in results['final_rewards'].items():\n",
    "    print(f\"  - {role}: {reward:.2f}\")\n",
    "print(f\"\\nüìà Total Squad Reward: {sum(results['final_rewards'].values()):.2f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot reward progression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for role, rewards in results['reward_history'].items():\n",
    "    ax.plot(range(len(rewards)), rewards, marker='o', label=role, linewidth=2, markersize=4)\n",
    "\n",
    "ax.set_xlabel('Timestep')\n",
    "ax.set_ylabel('Cumulative Reward')\n",
    "ax.set_title('Agent Reward Progression During Mission')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f57db",
   "metadata": {},
   "source": [
    "## 6. Custom Configuration\n",
    "\n",
    "Use YAML configuration for advanced customization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d89b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softkill9000.simulator import MissionSimulator\n",
    "from softkill9000.config.models import SimulationConfig, AgentConfig, MissionConfig, QLearningConfig\n",
    "\n",
    "# Define custom configuration with specific agent stats\n",
    "custom_config = SimulationConfig(\n",
    "    agents=[\n",
    "        AgentConfig(\n",
    "            role=\"Longsight\",\n",
    "            species=\"Vyr'khai\",\n",
    "            base_intelligence=85,\n",
    "            base_mobility=90,\n",
    "            base_tactical=80,\n",
    "            base_strength=55,\n",
    "            base_empathy=60\n",
    "        ),\n",
    "        AgentConfig(\n",
    "            role=\"Specter\",\n",
    "            species=\"Zephryl\",\n",
    "            base_strength=80,\n",
    "            base_tactical=85,\n",
    "            base_mobility=95,\n",
    "            base_intelligence=65,\n",
    "            base_empathy=50\n",
    "        ),\n",
    "        AgentConfig(\n",
    "            role=\"Lifebinder\",\n",
    "            species=\"Lumenari\",\n",
    "            base_empathy=95,\n",
    "            base_intelligence=80,\n",
    "            base_tactical=70,\n",
    "            base_strength=45,\n",
    "            base_mobility=65\n",
    "        )\n",
    "    ],\n",
    "    mission=MissionConfig(\n",
    "        num_timesteps=30,  # Longer mission\n",
    "        ethics_enabled=True\n",
    "    ),\n",
    "    q_learning=QLearningConfig(\n",
    "        episodes=1500,\n",
    "        gamma=0.92,\n",
    "        alpha=0.25,\n",
    "        epsilon=0.15\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è  Custom Configuration:\")\n",
    "print(f\"\\nüîß Agents:\")\n",
    "for agent in custom_config.agents:\n",
    "    print(f\"   - {agent.role} ({agent.species})\")\n",
    "    print(f\"     Stats: STR={agent.base_strength}, EMP={agent.base_empathy}, \"\n",
    "          f\"INT={agent.base_intelligence}, MOB={agent.base_mobility}, TAC={agent.base_tactical}\")\n",
    "\n",
    "print(f\"\\nüó∫Ô∏è  Mission Settings:\")\n",
    "print(f\"   - Timesteps: {custom_config.mission.num_timesteps}\")\n",
    "print(f\"   - Ethics: {'ENABLED' if custom_config.mission.ethics_enabled else 'DISABLED'}\")\n",
    "\n",
    "print(f\"\\nüéì Q-Learning Settings:\")\n",
    "print(f\"   - Episodes: {custom_config.q_learning.episodes}\")\n",
    "print(f\"   - Learning Rate: {custom_config.q_learning.alpha}\")\n",
    "print(f\"   - Discount Factor: {custom_config.q_learning.gamma}\")\n",
    "\n",
    "# Run custom simulation\n",
    "print(\"\\nüöÄ Running custom simulation...\")\n",
    "custom_sim = MissionSimulator(config=custom_config)\n",
    "custom_sim.setup()\n",
    "custom_results = custom_sim.run()\n",
    "\n",
    "print(\"\\nüìä Custom Simulation Results:\")\n",
    "print(f\"  Scenario: {custom_results['scenario']['description']}\")\n",
    "print(f\"  Location: {custom_results['scenario']['planet']} in {custom_results['scenario']['galaxy']}\")\n",
    "print(f\"\\n  Final Rewards:\")\n",
    "for role, reward in custom_results['final_rewards'].items():\n",
    "    print(f\"    - {role}: {reward:.2f}\")\n",
    "print(f\"\\n  Total Squad Reward: {sum(custom_results['final_rewards'].values()):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79203dab",
   "metadata": {},
   "source": [
    "## 7. Compare Multiple Scenarios\n",
    "\n",
    "Run multiple simulations and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61659257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softkill9000.simulator import MissionSimulator\n",
    "from softkill9000.config.models import SimulationConfig, AgentConfig, MissionConfig\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run multiple scenarios\n",
    "print(\"üîÑ Running multiple scenarios...\\n\")\n",
    "\n",
    "scenarios_data = []\n",
    "num_runs = 10\n",
    "\n",
    "# Standard agent configuration\n",
    "standard_agents = [\n",
    "    AgentConfig(role=\"Longsight\", species=\"Vyr'khai\"),\n",
    "    AgentConfig(role=\"Lifebinder\", species=\"Lumenari\"),\n",
    "    AgentConfig(role=\"Brawler\", species=\"Aetherborn\")\n",
    "]\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"  Run {run + 1}/{num_runs}...\", end=' ')\n",
    "    \n",
    "    # Create simulation with random scenario\n",
    "    config = SimulationConfig(\n",
    "        agents=standard_agents,\n",
    "        mission=MissionConfig(num_timesteps=20, ethics_enabled=True)\n",
    "    )\n",
    "    \n",
    "    sim = MissionSimulator(config=config)\n",
    "    sim.setup()\n",
    "    results = sim.run()\n",
    "    \n",
    "    total_reward = sum(results['final_rewards'].values())\n",
    "    avg_reward = total_reward / len(results['final_rewards'])\n",
    "    \n",
    "    scenarios_data.append({\n",
    "        'run': run + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'avg_reward': avg_reward,\n",
    "        'galaxy': results['scenario']['galaxy'],\n",
    "        'terrain': results['scenario']['terrain']\n",
    "    })\n",
    "    print(f\"Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "# Analyze results\n",
    "df = pd.DataFrame(scenarios_data)\n",
    "\n",
    "print(\"\\nüìä Statistical Summary:\")\n",
    "print(df[['total_reward', 'avg_reward']].describe())\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total reward bar chart\n",
    "axes[0].bar(df['run'], df['total_reward'], color='steelblue')\n",
    "axes[0].axhline(df['total_reward'].mean(), color='red', linestyle='--', label=f\"Mean: {df['total_reward'].mean():.2f}\")\n",
    "axes[0].set_xlabel('Run')\n",
    "axes[0].set_ylabel('Total Squad Reward')\n",
    "axes[0].set_title('Total Reward by Scenario')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average reward per agent\n",
    "axes[1].bar(df['run'], df['avg_reward'], color='green')\n",
    "axes[1].axhline(df['avg_reward'].mean(), color='red', linestyle='--', label=f\"Mean: {df['avg_reward'].mean():.2f}\")\n",
    "axes[1].set_xlabel('Run')\n",
    "axes[1].set_ylabel('Average Agent Reward')\n",
    "axes[1].set_title('Average Reward by Scenario')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Completed {num_runs} scenario runs\")\n",
    "print(f\"Total Reward: {df['total_reward'].mean():.2f} ¬± {df['total_reward'].std():.2f}\")\n",
    "print(f\"Avg per Agent: {df['avg_reward'].mean():.2f} ¬± {df['avg_reward'].std():.2f}\")\n",
    "\n",
    "# Show variety of scenarios encountered\n",
    "print(f\"\\nüåå Galaxies visited: {df['galaxy'].nunique()}\")\n",
    "print(f\"üåç Terrains encountered: {df['terrain'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3f89e3",
   "metadata": {},
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Explore more features:\n",
    "\n",
    "1. **REST API**: Deploy the FastAPI server for remote access\n",
    "2. **Custom Agents**: Create agents with specialized behaviors\n",
    "3. **Advanced Scenarios**: Design complex mission environments\n",
    "4. **Extended Training**: Train agents for more episodes to see performance improvements\n",
    "5. **Visualization**: Create custom plots and animations\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- [Architecture Guide](https://github.com/BkAsDrP/Softkill9000/blob/main/docs/architecture.md)\n",
    "- [API Reference](https://github.com/BkAsDrP/Softkill9000/blob/main/docs/api_reference.md)\n",
    "- [User Guide](https://github.com/BkAsDrP/Softkill9000/blob/main/docs/user_guide.md)\n",
    "- [Deployment Guide](https://github.com/BkAsDrP/Softkill9000/blob/main/docs/deployment.md)\n",
    "\n",
    "### Repository\n",
    "\n",
    "- GitHub: [BkAsDrP/Softkill9000](https://github.com/BkAsDrP/Softkill9000)\n",
    "- Issues: [Report bugs or request features](https://github.com/BkAsDrP/Softkill9000/issues)\n",
    "- License: MIT\n",
    "\n",
    "---\n",
    "\n",
    "**Happy simulating! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
